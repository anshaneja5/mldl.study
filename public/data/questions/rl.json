{
  "questions": [
    {
      "id": 1,
      "question": "What is reinforcement learning?",
      "options": [
        "A type of supervised learning",
        "A type of unsupervised learning",
        "A learning approach where agents learn by interacting with the environment to maximize rewards",
        "A type of feature engineering"
      ],
      "correctAnswer": 2,
      "explanation": "In reinforcement learning, agents learn by taking actions in an environment to maximize cumulative reward.",
      "difficulty": "easy"
    },
    {
      "id": 2,
      "question": "In reinforcement learning, what is an agent?",
      "options": [
        "A dataset",
        "An entity that explores and interacts with the environment",
        "A training algorithm",
        "A reward function"
      ],
      "correctAnswer": 1,
      "explanation": "An agent acts and learns from the consequences of its actions within an environment.",
      "difficulty": "easy"
    },
    {
      "id": 3,
      "question": "What is the role of the environment in reinforcement learning?",
      "options": [
        "It stores data",
        "It contains the agent's policy",
        "It provides states, rewards, and feedback for actions",
        "It optimizes hyperparameters"
      ],
      "correctAnswer": 2,
      "explanation": "The environment gives feedback to the agent via new states and rewards after each action.",
      "difficulty": "easy"
    },
    {
      "id": 4,
      "question": "What is a state in reinforcement learning?",
      "options": [
        "A value function",
        "A configuration of the environment at a particular time",
        "An action",
        "A reward"
      ],
      "correctAnswer": 1,
      "explanation": "The state represents the situation returned by the environment at any given step.",
      "difficulty": "easy"
    },
    {
      "id": 5,
      "question": "What does a reward represent in reinforcement learning?",
      "options": [
        "A measure of performance for an agent's action",
        "A label",
        "A penalty",
        "The policy"
      ],
      "correctAnswer": 0,
      "explanation": "A reward is feedback indicating how good or bad the agent's action is.",
      "difficulty": "easy"
    },
    {
      "id": 6,
      "question": "What is an action in reinforcement learning?",
      "options": [
        "A value function",
        "A step the agent can take within the environment",
        "A sequence of states",
        "A rollout"
      ],
      "correctAnswer": 1,
      "explanation": "Actions are the decisions or moves made by the agent at each step.",
      "difficulty": "easy"
    },
    {
      "id": 7,
      "question": "What is a policy in reinforcement learning?",
      "options": [
        "A function mapping states to actions",
        "A set of rewards",
        "A set of agents",
        "A neural network"
      ],
      "correctAnswer": 0,
      "explanation": "A policy guides the agent in selecting actions based on the current state.",
      "difficulty": "medium"
    },
    {
      "id": 8,
      "question": "What does exploration mean in reinforcement learning?",
      "options": [
        "Following the current optimal policy",
        "Trying new actions to discover more about the environment",
        "Maximizing reward based on known information",
        "Sampling more data"
      ],
      "correctAnswer": 1,
      "explanation": "Exploration allows the agent to try varied actions and learn better strategies.",
      "difficulty": "medium"
    },
    {
      "id": 9,
      "question": "What does exploitation mean in reinforcement learning?",
      "options": [
        "Trying random actions",
        "Using the best-known policy to maximize reward",
        "Increasing the dataset size",
        "Reducing exploration"
      ],
      "correctAnswer": 1,
      "explanation": "Exploitation uses current knowledge to achieve high rewards.",
      "difficulty": "medium"
    },
    {
      "id": 10,
      "question": "What is the exploration-exploitation tradeoff?",
      "options": [
        "Choosing only exploration",
        "Balancing learning new information and using current knowledge",
        "Using high learning rate",
        "Ignoring rewards"
      ],
      "correctAnswer": 1,
      "explanation": "The agent must decide between exploring new actions or exploiting existing knowledge for rewards.",
      "difficulty": "medium"
    },
    {
      "id": 11,
      "question": "Which algorithm forms the foundation for temporal-difference methods?",
      "options": [
        "Monte Carlo",
        "Q-learning",
        "K-means",
        "Backpropagation"
      ],
      "correctAnswer": 1,
      "explanation": "Q-learning is a well-known off-policy TD control algorithm.",
      "difficulty": "hard"
    },
    {
      "id": 12,
      "question": "What is the main objective of Q-learning?",
      "options": [
        "To minimize errors",
        "To learn the optimal action-value function",
        "To label data",
        "To encode categorical variables"
      ],
      "correctAnswer": 1,
      "explanation": "Q-learning learns the expected future rewards for actions given states.",
      "difficulty": "medium"
    },
    {
      "id": 13,
      "question": "What is value iteration?",
      "options": [
        "An unsupervised learning method",
        "An algorithm to update values of states towards the optimal value function",
        "A method for reward shaping",
        "A simulation approach"
      ],
      "correctAnswer": 1,
      "explanation": "Value iteration repeatedly updates the value function to estimate optimal policies.",
      "difficulty": "hard"
    },
    {
      "id": 14,
      "question": "What is policy iteration?",
      "options": [
        "Random search for policies",
        "An iterative algorithm that alternates between policy evaluation and improvement",
        "A clustering method",
        "Value function estimation only"
      ],
      "correctAnswer": 1,
      "explanation": "Policy iteration involves evaluating a policy and then improving it repeatedly.",
      "difficulty": "hard"
    },
    {
      "id": 15,
      "question": "What is the discount factor (gamma) in RL?",
      "options": [
        "Rewards immediate actions",
        "Discounts future rewards to prioritize present reward",
        "Increases state space",
        "Removes penalties"
      ],
      "correctAnswer": 1,
      "explanation": "Gamma (Î³) weighs future rewards to adjust agent's time preference.",
      "difficulty": "medium"
    },
    {
      "id": 16,
      "question": "What is a Markov Decision Process (MDP)?",
      "options": [
        "A clustering approach",
        "A tuple describing states, actions, transition function, and rewards",
        "A neural network model",
        "Unsupervised learning technique"
      ],
      "correctAnswer": 1,
      "explanation": "MDPs formally describe RL problems using states, actions, transitions, and rewards.",
      "difficulty": "medium"
    },
    {
      "id": 17,
      "question": "What is a state-action value function (Q-function)?",
      "options": [
        "Maps states to expected rewards",
        "Maps state-action pairs to expected cumulative rewards",
        "Maps rewards to states",
        "Aggregates policies"
      ],
      "correctAnswer": 1,
      "explanation": "Q-function provides expected future reward for taking a particular action at a given state.",
      "difficulty": "hard"
    },
    {
      "id": 18,
      "question": "What is temporal-difference (TD) learning?",
      "options": [
        "Uses samples to update values based on difference between estimates at successive time steps",
        "Before updating, waits until episode ends",
        "Updates action-value pairs using supervised labels",
        "Simulates random exploration"
      ],
      "correctAnswer": 0,
      "explanation": "TD learning updates value estimates based on the difference between successive predictions.",
      "difficulty": "hard"
    },
    {
      "id": 19,
      "question": "What is the difference between on-policy and off-policy learning?",
      "options": [
        "On-policy learns with the policy it follows; off-policy learns independently of the agent's actions",
        "Both are unsupervised",
        "Both require labeled data",
        "There is no difference"
      ],
      "correctAnswer": 0,
      "explanation": "On-policy methods update using actions of the agent; off-policy methods use actions from a different policy.",
      "difficulty": "hard"
    },
    {
      "id": 20,
      "question": "What is SARSA in reinforcement learning?",
      "options": [
        "An unsupervised clustering algorithm",
        "An on-policy TD control method",
        "A regression technique",
        "A reward adjustment process"
      ],
      "correctAnswer": 1,
      "explanation": "SARSA stands for State-Action-Reward-State-Action, updating Q-values using the agent's actions.",
      "difficulty": "medium"
    },
    {
      "id": 21,
      "question": "What do Monte Carlo methods do in RL?",
      "options": [
        "Use expectations over episodes to update estimates",
        "Require knowledge of transition probabilities",
        "Update after each time-step",
        "Always use off-policy learning"
      ],
      "correctAnswer": 0,
      "explanation": "Monte Carlo methods update estimates after complete episodes, averaging results.",
      "difficulty": "medium"
    },
    {
      "id": 22,
      "question": "What is a deterministic policy?",
      "options": [
        "A policy that outputs random actions",
        "A policy that always outputs the same action for a given state",
        "A policy based on supervised labels",
        "Returns reward directly"
      ],
      "correctAnswer": 1,
      "explanation": "Deterministic policies always choose the same action for a given state.",
      "difficulty": "easy"
    },
    {
      "id": 23,
      "question": "What is a stochastic policy?",
      "options": [
        "A policy that chooses actions randomly based on a distribution",
        "A policy without states",
        "A clustering technique",
        "A supervised learning algorithm"
      ],
      "correctAnswer": 0,
      "explanation": "Stochastic policies map states to distributions over possible actions.",
      "difficulty": "easy"
    },
    {
      "id": 24,
      "question": "What is the purpose of the epsilon-greedy strategy?",
      "options": [
        "Maximize exploitation only",
        "Balance exploration and exploitation by random action with probability epsilon",
        "Force agent to explore only",
        "Decrease learning rate"
      ],
      "correctAnswer": 1,
      "explanation": "Epsilon-greedy chooses random actions with probability epsilon, otherwise the best-known action.",
      "difficulty": "medium"
    },
    {
      "id": 25,
      "question": "What is a policy gradient method?",
      "options": [
        "Updates policies directly using gradients of expected reward",
        "Uses tabular methods for Q-values",
        "Updates rewards only",
        "Minimizes variance only"
      ],
      "correctAnswer": 0,
      "explanation": "Policy gradients optimize policies by adjusting parameters to maximize expected return.",
      "difficulty": "hard"
    },
    {
      "id": 26,
      "question": "What is the role of the value function in RL?",
      "options": [
        "Predict actions",
        "Evaluate expected future reward from a state or state-action pair",
        "Store rewards",
        "Define the policy"
      ],
      "correctAnswer": 1,
      "explanation": "Value functions quantify expected total future reward.",
      "difficulty": "medium"
    },
    {
      "id": 27,
      "question": "What is the Bellman Equation?",
      "options": [
        "A formula relating rewards, transitions, and values for optimal policies",
        "A loss function for training neural networks",
        "A method for calculating gradients",
        "A way to initialize states"
      ],
      "correctAnswer": 0,
      "explanation": "Bellman Equation provides recursive relationships for the optimal value function.",
      "difficulty": "hard"
    },
    {
      "id": 28,
      "question": "What are eligibility traces in RL?",
      "options": [
        "Store Q-value tables",
        "Memory constructs that allow credit assignment to previous state-action pairs",
        "Methods for policy evaluation",
        "A way to select rewards"
      ],
      "correctAnswer": 1,
      "explanation": "Eligibility traces bridge TD and Monte Carlo methods, crediting sequences of state-action pairs.",
      "difficulty": "hard"
    },
    {
      "id": 29,
      "question": "What is experience replay?",
      "options": [
        "Playing games to learn actions",
        "Storing and sampling past experiences for updates",
        "Orchestrating states only",
        "Randomizing rewards"
      ],
      "correctAnswer": 1,
      "explanation": "Experience replay buffers past experiences for more efficient learning and decorrelated updates.",
      "difficulty": "hard"
    },
    {
      "id": 30,
      "question": "What is the difference between episodic and continuing tasks?",
      "options": [
        "Episodic tasks terminate after fixed condition, continuing tasks run indefinitely",
        "Both run forever",
        "Neither can use reward",
        "Continuing tasks have discrete actions"
      ],
      "correctAnswer": 0,
      "explanation": "Episodic tasks have terminal states; continuing tasks run with an infinite time-horizon.",
      "difficulty": "medium"
    },
    {
      "id": 31,
      "question": "Which RL concept helps prevent overestimation of value functions?",
      "options": [
        "Double Q-learning",
        "SARSA",
        "Experience replay",
        "Monte Carlo methods"
      ],
      "correctAnswer": 0,
      "explanation": "Double Q-learning mitigates overestimation by separating action selection from value evaluation.",
      "difficulty": "hard"
    },
    {
      "id": 32,
      "question": "What is the function of a model in model-based RL?",
      "options": [
        "Estimates future states, rewards, and transitions based on current knowledge, enabling planning",
        "Stores Q-values",
        "Calculates gradients",
        "Tunes hyperparameters"
      ],
      "correctAnswer": 0,
      "explanation": "Model-based RL uses environment models to simulate and plan future actions.",
      "difficulty": "hard"
    },
    {
      "id": 33,
      "question": "What is the use of the actor-critic architecture?",
      "options": [
        "Combines policy (actor) and value function (critic) for improved learning efficiency",
        "Stores actions in tables",
        "Calculates variance",
        "Balances datasets"
      ],
      "correctAnswer": 0,
      "explanation": "Actor-critic methods use two structures: actor (policy) and critic (value estimator).",
      "difficulty": "hard"
    },
    {
      "id": 34,
      "question": "What is reward shaping?",
      "options": [
        "Modifying reward function to guide agent more effectively",
        "Increasing learning rate",
        "Reducing policy variance",
        "Altering state space"
      ],
      "correctAnswer": 0,
      "explanation": "Reward shaping accelerates learning by providing more informative feedback signals.",
      "difficulty": "medium"
    },
    {
      "id": 35,
      "question": "Which RL algorithm is suitable for continuous action spaces?",
      "options": [
        "DQN",
        "DDPG",
        "Monte Carlo",
        "Double Q-learning"
      ],
      "correctAnswer": 1,
      "explanation": "Deep Deterministic Policy Gradient (DDPG) handles continuous actions, unlike DQN.",
      "difficulty": "hard"
    },
    {
      "id": 36,
      "question": "What does the policy improvement step do?",
      "options": [
        "Evaluates current policy performance",
        "Generates a new policy expected to yield higher value based on evaluation",
        "Removes old rewards",
        "Samples random states"
      ],
      "correctAnswer": 1,
      "explanation": "Policy improvement updates the agent's policy based on evaluation for better expected reward.",
      "difficulty": "hard"
    },
    {
      "id": 37,
      "question": "What is a multi-armed bandit problem?",
      "options": [
        "A type of RL with multiple reward sources (arms) agent can select; focuses on exploration-exploitation",
        "Supervised classification task",
        "Clustering algorithm",
        "Classification by regression"
      ],
      "correctAnswer": 0,
      "explanation": "Multi-armed bandit focuses on maximizing reward in scenarios with several options.",
      "difficulty": "medium"
    },
    {
      "id": 38,
      "question": "What is transfer learning in RL?",
      "options": [
        "Using prior knowledge from one task to speed up learning in another related task",
        "Copying rewards from past actions",
        "Learning without feedback",
        "Training with random policies"
      ],
      "correctAnswer": 0,
      "explanation": "Transfer learning applies learned behaviors, policies, or models to new but similar tasks.",
      "difficulty": "medium"
    },
    {
      "id": 39,
      "question": "What is the credit assignment problem?",
      "options": [
        "Determining which actions led to observed rewards",
        "Balancing batches in training",
        "Updating weights",
        "Sampling new agents"
      ],
      "correctAnswer": 0,
      "explanation": "Credit assignment identifies which actions or states contributed to later rewards.",
      "difficulty": "hard"
    },
    {
      "id": 40,
      "question": "What is inverse reinforcement learning?",
      "options": [
        "Learning a reward function by observing expert behaviors",
        "Learning from labeled data",
        "Optimizing a fixed reward function",
        "Ignoring the environment"
      ],
      "correctAnswer": 0,
      "explanation": "Inverse RL infers the underlying reward structure from observed demonstrations.",
      "difficulty": "hard"
    }
  ]
}
