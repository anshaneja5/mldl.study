{
  "questions": [
    {
      "id": 1,
      "question": "What is a neuron in a neural network?",
      "options": [
        "A biological cell",
        "A computational unit that applies weights and activation function",
        "A type of layer",
        "A training algorithm"
      ],
      "correctAnswer": 1,
      "explanation": "A neuron is a computational unit that takes inputs, applies weights and bias, then passes through an activation function.",
      "difficulty": "easy"
    },
    {
      "id": 2,
      "question": "What is the purpose of an activation function?",
      "options": [
        "Initialize weights",
        "Introduce non-linearity",
        "Calculate loss",
        "Update weights"
      ],
      "correctAnswer": 1,
      "explanation": "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns.",
      "difficulty": "medium"
    },
    {
      "id": 3,
      "question": "Which optimizer uses momentum?",
      "options": [
        "SGD with momentum",
        "Adam",
        "RMSprop",
        "All of the above"
      ],
      "correctAnswer": 3,
      "explanation": "SGD, Adam, and RMSprop all use some form of momentum to accelerate convergence.",
      "difficulty": "medium"
    },
    {
      "id": 4,
      "question": "What is backpropagation?",
      "options": [
        "Forward pass through the network",
        "Algorithm for calculating gradients and updating weights",
        "Type of activation function",
        "Loss function"
      ],
      "correctAnswer": 1,
      "explanation": "Backpropagation calculates gradients of the loss with respect to weights and propagates errors backward.",
      "difficulty": "medium"
    },
    {
      "id": 5,
      "question": "Which activation function is commonly used in hidden layers of deep networks?",
      "options": [
        "Sigmoid",
        "ReLU",
        "Softmax",
        "Tanh"
      ],
      "correctAnswer": 1,
      "explanation": "ReLU (Rectified Linear Unit) is widely used because it helps mitigate vanishing gradient problems.",
      "difficulty": "easy"
    },
    {
      "id": 6,
      "question": "What is the main purpose of dropout in deep learning?",
      "options": [
        "Increase training speed",
        "Prevent overfitting",
        "Initialize weights",
        "Improve gradient flow"
      ],
      "correctAnswer": 1,
      "explanation": "Dropout randomly deactivates neurons during training to prevent overfitting.",
      "difficulty": "medium"
    },
    {
      "id": 7,
      "question": "Which loss function is commonly used for multi-class classification?",
      "options": [
        "Mean Squared Error",
        "Binary Cross-Entropy",
        "Categorical Cross-Entropy",
        "Hinge Loss"
      ],
      "correctAnswer": 2,
      "explanation": "Categorical Cross-Entropy measures the difference between predicted and true probability distributions for multi-class classification.",
      "difficulty": "medium"
    },
    {
      "id": 8,
      "question": "What does CNN stand for?",
      "options": [
        "Cognitive Neural Network",
        "Convolutional Neural Network",
        "Central Neural Node",
        "Conditional Network"
      ],
      "correctAnswer": 1,
      "explanation": "CNN stands for Convolutional Neural Network, primarily used for image-related tasks.",
      "difficulty": "easy"
    },
    {
      "id": 9,
      "question": "What is the main advantage of using batch normalization?",
      "options": [
        "Faster convergence and stable training",
        "Increase model depth",
        "Reduce memory usage",
        "Increase overfitting"
      ],
      "correctAnswer": 0,
      "explanation": "Batch normalization normalizes layer inputs, stabilizing and speeding up training.",
      "difficulty": "medium"
    },
    {
      "id": 10,
      "question": "Which architecture is best suited for sequential data like text or time series?",
      "options": [
        "CNN",
        "RNN",
        "Autoencoder",
        "GAN"
      ],
      "correctAnswer": 1,
      "explanation": "RNNs (Recurrent Neural Networks) are designed to handle sequential dependencies in data.",
      "difficulty": "medium"
    },
    {
      "id": 11,
      "question": "In deep learning, what is a vanishing gradient problem?",
      "options": [
        "Gradients become zero, slowing or stopping learning",
        "Gradients explode exponentially",
        "Loss becomes undefined",
        "Weights are not updated at all"
      ],
      "correctAnswer": 0,
      "explanation": "Vanishing gradients occur when gradients become too small during backpropagation, preventing weight updates in deep layers.",
      "difficulty": "hard"
    },
    {
      "id": 12,
      "question": "What does LSTM stand for?",
      "options": [
        "Long Short-Term Memory",
        "Linear Short-Term Model",
        "Limited Sequence Training Model",
        "Local Sequential Training Mechanism"
      ],
      "correctAnswer": 0,
      "explanation": "LSTM stands for Long Short-Term Memory, a type of RNN designed to capture long-term dependencies.",
      "difficulty": "easy"
    },
    {
      "id": 13,
      "question": "What is the discriminator's role in a GAN?",
      "options": [
        "Generate fake data",
        "Distinguish real data from fake data",
        "Compute gradients",
        "Reduce model size"
      ],
      "correctAnswer": 1,
      "explanation": "The discriminator in a GAN learns to differentiate between real and fake samples produced by the generator.",
      "difficulty": "medium"
    },
    {
      "id": 14,
      "question": "Which layer type in CNNs helps reduce spatial dimensions?",
      "options": [
        "Fully Connected Layer",
        "Convolutional Layer",
        "Pooling Layer",
        "Recurrent Layer"
      ],
      "correctAnswer": 2,
      "explanation": "Pooling layers (e.g., max pooling) reduce the spatial dimensions, helping to extract key features efficiently.",
      "difficulty": "easy"
    },
    {
      "id": 15,
      "question": "What is transfer learning?",
      "options": [
        "Training multiple networks simultaneously",
        "Using a pre-trained model on a new but related task",
        "Sharing data between models",
        "Training without backpropagation"
      ],
      "correctAnswer": 1,
      "explanation": "Transfer learning leverages knowledge from a pre-trained model to solve a new, similar problem efficiently.",
      "difficulty": "medium"
    },
    {
      "id": 16,
      "question": "What is the primary advantage of using Batch Normalization in deep neural networks?",
      "options": [
        "It reduces the number of parameters",
        "It normalizes layer inputs, stabilizing training and allowing higher learning rates",
        "It eliminates the need for activation functions",
        "It automatically selects optimal architecture"
      ],
      "correctAnswer": 1,
      "explanation": "Batch Normalization normalizes inputs to each layer, reducing internal covariate shift, stabilizing gradients, and allowing faster training with higher learning rates.",
      "difficulty": "medium"
    },
    {
      "id": 17,
      "question": "In a Transformer architecture, what is the purpose of the attention mechanism?",
      "options": [
        "To reduce model size",
        "To allow the model to focus on different parts of the input sequence when processing each element",
        "To prevent overfitting",
        "To eliminate the need for embeddings"
      ],
      "correctAnswer": 1,
      "explanation": "Self-attention allows each position in a sequence to attend to all positions, capturing long-range dependencies more effectively than RNNs.",
      "difficulty": "hard"
    },
    {
      "id": 18,
      "question": "What problem does Residual Connection (Skip Connection) solve in very deep networks?",
      "options": [
        "It reduces memory usage",
        "It allows gradients to flow directly through the network, mitigating vanishing gradients",
        "It increases the number of parameters",
        "It eliminates the need for normalization"
      ],
      "correctAnswer": 1,
      "explanation": "Residual connections (as in ResNet) create shortcut paths that allow gradients to bypass layers, enabling training of much deeper networks (100+ layers).",
      "difficulty": "hard"
    },
    {
      "id": 19,
      "question": "What is the main difference between GRU and LSTM architectures?",
      "options": [
        "GRU has fewer gates (2) than LSTM (3), making it computationally more efficient",
        "GRU can only process sequences forward, LSTM can process bidirectionally",
        "LSTM is always more accurate than GRU",
        "GRU requires more training data than LSTM"
      ],
      "correctAnswer": 0,
      "explanation": "GRU combines forget and input gates into an update gate, and merges cell state with hidden state, resulting in fewer parameters and faster training.",
      "difficulty": "medium"
    },
    {
      "id": 20,
      "question": "In CNNs, what does a 1x1 convolution primarily accomplish?",
      "options": [
        "It increases spatial dimensions",
        "It reduces or changes the number of channels/feature maps while maintaining spatial dimensions",
        "It replaces pooling layers",
        "It adds non-linearity without parameters"
      ],
      "correctAnswer": 1,
      "explanation": "1x1 convolutions perform channel-wise operations, reducing dimensionality (like PCA for channels) while adding non-linearity, used extensively in architectures like Inception and MobileNet.",
      "difficulty": "hard"
    },
    {
      "id": 21,
      "question": "What is the purpose of the decoder in an autoencoder?",
      "options": [
        "To classify the input data",
        "To reconstruct the input from the compressed latent representation",
        "To generate labels for unsupervised data",
        "To perform dimensionality reduction only"
      ],
      "correctAnswer": 1,
      "explanation": "The decoder takes the compressed latent representation from the encoder and attempts to reconstruct the original input, learning meaningful representations through this process.",
      "difficulty": "easy"
    },
    {
      "id": 22,
      "question": "What distinguishes a Variational Autoencoder (VAE) from a standard autoencoder?",
      "options": [
        "VAE has more layers",
        "VAE learns a probabilistic distribution in latent space, enabling generation of new samples",
        "VAE only works on images",
        "VAE requires labeled data"
      ],
      "correctAnswer": 1,
      "explanation": "VAE encodes inputs as probability distributions (mean and variance) rather than fixed points, allowing sampling from latent space to generate new, similar data.",
      "difficulty": "hard"
    },
    {
      "id": 23,
      "question": "In a Generative Adversarial Network (GAN), what is the discriminator's objective?",
      "options": [
        "To generate realistic fake data",
        "To distinguish between real and generated (fake) data",
        "To compress input data",
        "To classify data into multiple categories"
      ],
      "correctAnswer": 1,
      "explanation": "The discriminator acts as a binary classifier, learning to differentiate real samples from fake ones generated by the generator, creating an adversarial training dynamic.",
      "difficulty": "medium"
    },
    {
      "id": 24,
      "question": "What is transfer learning in the context of deep learning?",
      "options": [
        "Training a model from scratch on a new dataset",
        "Using pre-trained model weights as initialization for a new task with similar features",
        "Transferring data between training and test sets",
        "Converting models between different frameworks"
      ],
      "correctAnswer": 1,
      "explanation": "Transfer learning leverages knowledge from models pre-trained on large datasets (like ImageNet), fine-tuning them for new tasks with less data and computation.",
      "difficulty": "easy"
    },
    {
      "id": 25,
      "question": "What does the receptive field refer to in CNNs?",
      "options": [
        "The total number of parameters in the network",
        "The region of the input image that influences a particular feature in a deeper layer",
        "The size of the convolutional kernel",
        "The number of filters in a layer"
      ],
      "correctAnswer": 1,
      "explanation": "Receptive field is the area of the input that affects a neuron's activation. Deeper layers have larger receptive fields, capturing more global features.",
      "difficulty": "medium"
    },
    {
      "id": 26,
      "question": "What is the role of the generator in a GAN?",
      "options": [
        "To classify real vs fake images",
        "To create synthetic data that resembles the real data distribution",
        "To reduce dimensionality of input",
        "To provide labels for training"
      ],
      "correctAnswer": 1,
      "explanation": "The generator takes random noise as input and learns to produce realistic samples that can fool the discriminator, improving through adversarial training.",
      "difficulty": "easy"
    },
    {
      "id": 27,
      "question": "What is the purpose of positional encoding in Transformer models?",
      "options": [
        "To reduce model complexity",
        "To inject information about token positions since self-attention is order-agnostic",
        "To normalize input values",
        "To prevent overfitting"
      ],
      "correctAnswer": 1,
      "explanation": "Unlike RNNs, self-attention has no inherent notion of sequence order. Positional encodings (sine/cosine or learned) add position information to token embeddings.",
      "difficulty": "hard"
    },
    {
      "id": 28,
      "question": "What is gradient clipping and why is it used?",
      "options": [
        "A technique to speed up training",
        "A method to prevent exploding gradients by capping gradient values during backpropagation",
        "A way to reduce model size",
        "A regularization technique for overfitting"
      ],
      "correctAnswer": 1,
      "explanation": "Gradient clipping limits gradient magnitudes (by value or norm) to prevent exploding gradients in RNNs and deep networks, ensuring stable training.",
      "difficulty": "medium"
    },
    {
      "id": 29,
      "question": "What is the main advantage of depthwise separable convolutions (used in MobileNet)?",
      "options": [
        "They increase model accuracy significantly",
        "They reduce computational cost and parameters by factorizing standard convolutions",
        "They eliminate the need for activation functions",
        "They only work on small images"
      ],
      "correctAnswer": 1,
      "explanation": "Depthwise separable convolutions split standard convolution into depthwise (spatial) and pointwise (1x1) operations, reducing computation by ~8-9x with minimal accuracy loss.",
      "difficulty": "hard"
    },
    {
      "id": 30,
      "question": "What is the purpose of multi-head attention in Transformers?",
      "options": [
        "To reduce training time",
        "To allow the model to attend to information from different representation subspaces simultaneously",
        "To increase the vocabulary size",
        "To eliminate positional encoding"
      ],
      "correctAnswer": 1,
      "explanation": "Multi-head attention runs multiple attention mechanisms in parallel, each learning different aspects of relationships (e.g., syntax, semantics), then concatenates results.",
      "difficulty": "hard"
    },
    {
      "id": 31,
      "question": "Why are activation functions important in neural networks?",
      "options": [
        "They add linearity to the network",
        "They introduce non-linearity allowing the network to learn complex patterns",
        "They help in weight initialization",
        "They prevent overfitting"
      ],
      "correctAnswer": 1,
      "explanation": "Activation functions introduce non-linearity, enabling networks to approximate complex functions beyond linear mappings.",
      "difficulty": "easy"
    },
    {
      "id": 32,
      "question": "What is the main limitation of the sigmoid activation function?",
      "options": [
        "It outputs values only between -1 and 1",
        "It is computationally expensive",
        "It causes vanishing gradient problem for deep networks",
        "It is not differentiable"
      ],
      "correctAnswer": 2,
      "explanation": "Sigmoid saturates at extreme values, causing very small gradients and slowing learning in deep networks.",
      "difficulty": "medium"
    },
    {
      "id": 33,
      "question": "How does the ReLU activation function work?",
      "options": [
        "Outputs 0 for negative inputs, outputs input as is for positive inputs",
        "Outputs values between 0 and 1",
        "Outputs values between -1 and 1",
        "Outputs a constant value"
      ],
      "correctAnswer": 0,
      "explanation": "ReLU outputs zero for all negative inputs, and outputs the input directly if positive, helping with sparse activations and mitigating vanishing gradients.",
      "difficulty": "easy"
    },
    {
      "id": 34,
      "question": "What is the 'dying ReLU' problem?",
      "options": [
        "ReLU neurons output zero for all inputs due to negative summed input",
        "ReLU neurons explode in value",
        "ReLU is not differentiable",
        "ReLU causes overfitting"
      ],
      "correctAnswer": 0,
      "explanation": "In dying ReLU, neurons output zero consistently if weights update causes inputs to remain negative, effectively disabling those neurons.",
      "difficulty": "hard"
    },
    {
      "id": 35,
      "question": "Which activation function is recommended for output layer in multi-class classification?",
      "options": [
        "ReLU",
        "Sigmoid",
        "Softmax",
        "Tanh"
      ],
      "correctAnswer": 2,
      "explanation": "Softmax converts raw output scores into a probability distribution across classes, suited for multi-class tasks.",
      "difficulty": "medium"
    },
    {
      "id": 36,
      "question": "What property does the Tanh activation function have compared to Sigmoid?",
      "options": [
        "Outputs range between 0 and 1",
        "Outputs range between -1 and 1 centered around zero",
        "Is not differentiable",
        "Has constant gradient"
      ],
      "correctAnswer": 1,
      "explanation": "Tanh squashes inputs to be between -1 and 1, outputting zero-centered responses which can help optimization.",
      "difficulty": "medium"
    },
    {
      "id": 37,
      "question": "How does batch normalization help neural networks?",
      "options": [
        "By normalizing layer inputs across batches to reduce internal covariate shift and stabilize training",
        "By applying dropout to layers",
        "By reducing the number of parameters",
        "By increasing the learning rate"
      ],
      "correctAnswer": 0,
      "explanation": "Batch normalization standardizes inputs to each layer, accelerating training and allowing higher learning rates.",
      "difficulty": "medium"
    },
    {
      "id": 38,
      "question": "Why are Leaky ReLU or Parametric ReLU variants used instead of regular ReLU?",
      "options": [
        "To allow small gradient for negative inputs preventing dying neurons",
        "To improve speed",
        "To make the function non-differentiable",
        "To reduce overfitting"
      ],
      "correctAnswer": 0,
      "explanation": "Leaky ReLU variants provide a small, non-zero gradient when inputs are negative, avoiding neurons permanently shutting off.",
      "difficulty": "medium"
    },
    {
      "id": 39,
      "question": "Which activation function is often used in the hidden layers of deep networks to avoid vanishing gradients?",
      "options": [
        "Sigmoid",
        "Tanh",
        "ReLU",
        "Linear"
      ],
      "correctAnswer": 2,
      "explanation": "ReLU and its variants help maintain non-zero gradients and enable effective learning in deep architectures.",
      "difficulty": "easy"
    },
    {
      "id": 40,
      "question": "What is the main drawback of the Linear activation function in hidden layers?",
      "options": [
        "It introduces non-linearity",
        "It causes vanishing gradients",
        "It restricts the network to linear transformations only, regardless of depth",
        "It is expensive to compute"
      ],
      "correctAnswer": 2,
      "explanation": "Using linear activations everywhere reduces a multi-layer network to effectively a single-layer linear model.",
      "difficulty": "medium"
    }
  ]
}